// Copyright 2014 Thomas MÃ¼ller
// This file is part of MarMoT, which is licensed under GPLv3.

package marmot.tokenize.openlp;

import java.io.FileInputStream;
import java.io.IOException;
import java.nio.charset.Charset;
import java.util.regex.Pattern;

import marmot.tokenize.Tokenizer;
import opennlp.model.TrainUtil;
import opennlp.tools.dictionary.Dictionary;
import opennlp.tools.tokenize.TokenSample;
import opennlp.tools.tokenize.TokenSampleStream;
import opennlp.tools.tokenize.TokenizerFactory;
import opennlp.tools.tokenize.TokenizerME;
import opennlp.tools.tokenize.TokenizerModel;
//import opennlp.tools.util.InputStreamFactory;
import opennlp.tools.util.ObjectStream;
import opennlp.tools.util.PlainTextByLineStream;
import opennlp.tools.util.TrainingParameters;

public class OpenNlpTokenizerTrainer {
	
	public final static int CUTOFF = 1;
	/*
	public Tokenizer train(String path) throws IOException {
		TokenizerModel model;

		ObjectStream<String> line_stream = new PlainTextByLineStream(
				(InputStreamFactory) new FileInputStream(path), Charset.forName("UTF-8"));
		ObjectStream<TokenSample> samples = new TokenSampleStream(line_stream);

		String lang_code = "";
		Dictionary dict = null;
		Pattern alpha_numeric_pattern = null;
		
		TokenizerFactory factory = new TokenizerFactory(lang_code, dict, true, alpha_numeric_pattern);
		
		TrainingParameters params = TrainingParameters.defaultParams();
		params.put(TrainUtil.CUTOFF_PARAM, Integer.toString(CUTOFF));
				
		try {
			model = TokenizerME.train(samples, factory, params);
		}
		finally {
		  samples.close();
		}
		
		return new OpenNlpTokenizer(model);		
	}
	
	public static void main(String[] args) throws IOException {
		OpenNlpTokenizerTrainer trainer = new OpenNlpTokenizerTrainer();
		Tokenizer tokenizer = trainer.train("/mounts/data/proj/marmot/tokenizer/data/en/test.txt");
		tokenizer.saveToFile("/mounts/data/proj/marmot/tokenizer/data/en/test.txt.tok");
		
		System.out.println(tokenizer.tokenize("Shouldn't this work, now?"));
	}
	*/
}
